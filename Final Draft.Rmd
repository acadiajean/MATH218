---
title: "Final Project \"Before\" and \"After\""
author: "Acadia Hegedus"
date: "12/10/2021"
output: html_document
---

##"Before"
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE)
```

Load in relevant libraries. 

```{r}
library(data.table)
library(rvest)
library(stringr)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(readxl)
library(class)
library(FNN)
library(MASS)
library(ISLR)
library(rpart)
library(rattle)
library(ipred)
library(randomForest)
library(caret)
library(spotifyr)
library(dendextend)
```
Read in our data set.  
```{r}
wind_power <- read_csv("Turbine_Data.csv")
wind.power <- wind_power
wind.power <- wind.power%>%
  na.omit()
wind.power <- wind.power%>% #mutate a tracking ID column so I can filter the correct dates
  mutate(tracking_id = seq(1, nrow(wind.power),1))
colnames(wind.power)[1] <- "datetime"

```

  Our data set comes from https://www.kaggle.com/theforcecoder/wind-power-forecasting. Included in the data set are various weather, wind turbine, and rotor variables, in addition to the power generated, for a single windmill. We hope to predict how much power this windmill will produce given various readings from sensors on the windmill and the weather. We assume the data were collected using various measurements devices. We are not sure geographically where the windmill is.
  Variables in this data set are: the tracking_id (which now has a 1:1 correspondence with the time stamp), a time stamp, the power generated by the windmill, the ambient temperature, the temperatures of parts of the windmill including the bearing shaft, gearbox bearing, gearbox oil, 2 generator windings, hub, and the main box. The data set also has the pitch angles of each blade of the wind turbine, generator RPM, the nacelle position (the direction the windmill is pointing), reactive power (used to regulate the voltage), rotor RPM, turbine status (?), the unique ID on the windmill (WTG), wind direction, and wind speed. 
  All variables are quantitative except for tracking_id, datetime, turbine_status, and WTG, which are qualitative variables. 
  Our "before" data is data read from May 2019 through October 2019. Our "after" data is data read November 2019 - March 2020. 

```{r}
before.wind <- wind.power%>%
  filter(tracking_id < 13654)

after.wind <- wind.power%>%
  filter(tracking_id > 13653)
```

Let's explore our "before" wind data. 

First, let's determine what the units on the ActivePower column are.

An average wind turbine might produce ~ 550 kW. (Source: https://www.usgs.gov/faqs/how-much-wind-energy-does-it-take-power-average-home?qt-news_science_products=0#qt-news_science_products).

The wind power generated in this data set takes a range from 0 to around 1750. Let's assume the ActivePower column has units of kW.
```{r}
before.wind %>% 
  ggplot(mapping = aes(x = tracking_id, y = `ActivePower`)) +
  geom_point()

```
We wish to predict this power generation column.  

To explore which of the predictor variables will be relevant when building our
supervised learning model, we explore the underlying structure of the data set using unsupervised learning methods.

First, we use Principal Components Analysis to see which variables might be the most important for our model. We have 23 columns in our data set. This includes the response variable, datetime, tracking_id, and WTG, which all will be useless for use in a model. (WTG has only one value, as this is data from a single windmill). This leaves a total of 19 possible variables to choose from. Instead of using all of the possible variables, it makes sense to only choose variables that are uncorrelated from each other. This will allow our model to predict more effectively. 

Let's only explore the quantitative variables in our data set, leaving out turbine status. This leaves us with 18 quantitative variables to explore.

Before we conduct PCA, we notice a few variables that we can get rid of right away. The control box temperature variable appears to be a single value (of zero), and the nacelle position is equivalent to the wind direction variable, as the windturbine always turns to the direction of the wind.
```{r}
before.wind %>%
  ggplot(mapping = aes(x = tracking_id, y = ControlBoxTemperature))+
  geom_point()

before.wind %>%
  ggplot(mapping = aes(x = NacellePosition, y = WindDirection))+
  geom_point()
```

So, we will leave out nacelle position and control box temperature from our model as well. Now we are down to 16 variables that we may use to model power generation.


```{r}
before.wind.vars <- before.wind%>%
  dplyr::select(WindSpeed,WindDirection,
                             AmbientTemperatue, BearingShaftTemperature, Blade1PitchAngle, Blade2PitchAngle, Blade3PitchAngle, GearboxBearingTemperature, GearboxOilTemperature,  
GeneratorRPM, GeneratorWinding1Temperature,
GeneratorWinding2Temperature, HubTemperature, MainBoxTemperature,            
ReactivePower, RotorRPM)

pca1 <- prcomp(before.wind.vars, scale = TRUE)
biplot(pca1)
pca1
```

From these initial PCA results using all of our possible variables, we can see that Blade1PitchAngle, Blade2PitchAngle, and Blade3PitchAngle are extremely highly correlated. So, let's only use Blade1PitchAngle in our model. GeneratorWinding1Temperature and GeneratorWinding2Temperature are also essentially the same, so let's only use GeneratorWinding1Temperature in our model. RotorRPM and GeneratorRPM also seem to be very highly correlated, so I will just use GeneratorRPM. 

```{r}
before.wind %>%
  ggplot(mapping = aes(x = Blade1PitchAngle, y = Blade2PitchAngle, color = Blade3PitchAngle))+
  geom_point()

before.wind %>%
  ggplot(mapping = aes(x = GeneratorWinding1Temperature, y = GeneratorWinding2Temperature))+
  geom_point()

before.wind %>%
  ggplot(mapping = aes(x = RotorRPM, y = GeneratorRPM))+
  geom_point()
```

Let's do another PCA now that we have gotten rid of variables that are extremely highly correlated. 

```{r}
before.wind.vars <- before.wind%>%
  dplyr::select(WindSpeed,WindDirection,
                             AmbientTemperatue, BearingShaftTemperature, Blade1PitchAngle, GearboxBearingTemperature, GearboxOilTemperature,  
GeneratorRPM, GeneratorWinding1Temperature, HubTemperature, MainBoxTemperature, 
ReactivePower)

pca1 <- prcomp(before.wind.vars, scale = TRUE)
biplot(pca1)
pca1
```

The only pair of variables left that may be extremely highly correlated are GearboxBearing and GearboxOilTemperature, from looking at the almost identical PC1 values.

```{r}
before.wind %>%
  ggplot(mapping = aes(x = GearboxBearingTemperature, y = GearboxOilTemperature))+
  geom_point()
```

Although GearboxBearing and GearboxOilTemperature look positively correlated, the correlation doesn't look strong enough (relative to the first few variables we got rid of) to get rid of one. So, let's keep both. 

So, the quantitative data variables that I will make my model with will be: WindSpeed, WindDirection, AmbientTemperatue, BearingShaftTemperature, Blade1PitchAngle, GearboxBearingTemperature, GearboxOilTemperature, GeneratorRPM, GeneratorWinding1Temperature, HubTemperature, MainBoxTemperature, and
ReactivePower. This is 12 quantitative variables as opposed to 16, which seems reasonable.

```{r}
before.wind <- before.wind%>%
  dplyr::select(WindSpeed, WindDirection, AmbientTemperatue, BearingShaftTemperature, Blade1PitchAngle, GearboxBearingTemperature, GearboxOilTemperature, GeneratorRPM, GeneratorWinding1Temperature, HubTemperature, MainBoxTemperature, ReactivePower, datetime, tracking_id, ActivePower, TurbineStatus)
```

Next, we use hierarchical clustering to identify if there are any outliers in our data. We use this over k-means clustering as we have no intuition on the optimal number of clusters. Again, we only use the quantitative variables here.  

```{r}
hc1 <- before.wind %>%
  dplyr::select(-c(tracking_id, datetime, TurbineStatus, ActivePower))%>%
  scale()%>%
  dist()%>%
  hclust()

before.wind.clusters <- before.wind%>%
  mutate(cluster = cutree(hc1, k = 5))

hist(before.wind.clusters$cluster)
```

It looks like, when choosing an arbitrary k = 5, almost all of the data points fall within clusters 1 and 3. What is happening in clusters 2, 4, & 5? Do these contain outlier points that need to be omitted?

```{r}
before.wind.clusters%>%
  group_by(cluster)%>%
  summarize_all(mean)

#How many points are in each cluster?
before.wind.clusters%>%
  filter(cluster == 1)%>%
  nrow()

before.wind.clusters%>%
  filter(cluster == 2)%>%
  nrow()

before.wind.clusters%>%
  filter(cluster == 3)%>%
  nrow()

before.wind.clusters%>%
  filter(cluster == 4)%>%
  nrow()

before.wind.clusters%>%
  filter(cluster == 5)%>%
  nrow()
```

It looks like there are 16 data points in cluster 2, 18 in cluster 4, and only 1 in cluster 5. 

Cluster 2 seems to have a very low relative bearing shaft temperature, low gearbox bearing temperature, low hub temperature, and low main box temperature. The cluster's average temperatures of the gearbox, bearing shaft, and hub are all zero. Given that the generator RPM is nonzero, we know the wind turbine must be generating some power. This is confirmed by the active power variable being nonzero. I'm going to assume that these zero temperature readings are a malfunction of the sensors and remove cluster 2 from my data. 

```{r}
before.wind.clusters%>%
  filter(cluster == 2)%>%
  ggplot(mapping = aes(x = tracking_id, y = GearboxBearingTemperature))+
  geom_point()
```

Besides having a relatively low hub temperature and main box temperatures, Cluster 4 does not seem to have any average variables that are completely out of the question. So, we keep this cluster in our data.  

Our cluster 5 data point seems to have many zero (0.000) values (including for the generator RPM). We are going to get rid of this outlier, as having a single data point in its own cluster, given the data set size and k value, hints that this data point is in fact an outlier. 

```{r}
before.wind <- before.wind.clusters%>%
  filter(cluster != 2)%>%
  filter(cluster != 5)
```

Now that we know which variables we are going to use in our model and we have gotten rid of outliers in our data set, let's construct two separate supervised learning models, to predict the power generated by this windmill. Since we are predicting a quantitative variable, we chose to use a kNN model and a random forest. 

Let's build a kNN model first. To build the best kNN model possible with our given data set, we use k-fold cross validation to identify the most ideal value of k that minimizes error. We define error as (predicted-actual)^2. We use k-fold cross validation to save on computation time, given the large size of our data set. I use 5 folds for the cross validation. 

Since we are using kNN and have to calculate a distance metric, we only use the quantitative variables in before.wind. 

```{r}
before.wind.kNN <- before.wind%>%
  dplyr::select(-datetime,-tracking_id,-TurbineStatus,-cluster)
```


```{r}
folds <- 5
fold.error.vector <- NULL
error.vector <- NULL

wind.data.groups <- split(before.wind.kNN, sample(1:folds, nrow(before.wind.kNN), replace=T))

for (k in 1:100){
  for (fold in 1:folds){
    myfold <- data.frame(wind.data.groups[fold])
    colnames(myfold) <- colnames(before.wind.kNN)
    test.data <- myfold%>%
      subset(select = -ActivePower)
    training.data <- before.wind.kNN[-as.numeric(rownames(test.data)),]
    training.data.no.power <- training.data %>% 
      subset(select = -ActivePower)

    model1 <- knn.reg(train = training.data.no.power, 
                          test = test.data, 
                          k = k, y = training.data$ActivePower)
    test.data <- test.data%>%
      mutate(error = (myfold$ActivePower - model1$pred)^2)
    fold.error.vector[fold] <- mean(test.data$error) #stores average error value for a given fold
  }
  error.vector[k] <- mean(fold.error.vector) #stores average of average error value of all folds for a given k
}
```

```{r}
pred.data <- data.frame(error.vector, 
                        k = 1:100)
pred.data %>%
  ggplot(aes(x = k, 
             y = error.vector)) +
  geom_point() +
  ggtitle("Error vs. K using kNN") +
  xlab("K Value") +
  ylab("Error") +
  theme(plot.title = element_text(hjust = 0.5))

k.best <- which.min(error.vector)
error.best <- pred.data$error.vector[k.best]
print(paste("The best k value to use is k =",as.character(k.best),"with a minimal mean error of",as.character(round(error.best))))
```

Now, let's build a random forest to predict power generation. To do this, let's again minimize error, as we are predicting a quantitative response variable.

We will use all possible variables to build this random forest. This includes all of the variables that we used for kNN, plus the TurbineStatus variable.
```{r}
before.wind.rf <- before.wind%>%
  dplyr::select(-datetime,-tracking_id,-cluster)
```

To optimize our random forest, we will need to pick the best hyperparameters including mtry (the number of variables for each bagged regression tree to consider), node size (the minimum size of a terminal node), and bootstrap resample size. We fix ntree (the number of trees in our random forest) to be 100 and optimize it later, as we know more trees in our forest will only increase the model's performance. We define error here as the out-of-bag mean squared error of the random forest's predictions. 

```{r}
# metric.data <- data.frame(0,0,0,0)
# colnames(metric.data) <- c("i", "j", "k", "error")
# 
# for (i in 1:13){ #range of mtry 
#   for (j in 1:10){ #range of nodesizes ADAPT THIS RANGE?
#     for (k in seq(7636,13636, by = 3000)){ #range of bootstrap resample sizes
#       rf <- randomForest(ActivePower ~ .,
#                   data = before.wind.rf, mtry = i, nodesize = j,sampsize = k, ntree = 100) #this is lower than we would like, but we will change later
# 
#       error <- tail(rf$mse, 1) #this came from https://stats.stackexchange.com/questions/369134/random-forest-out-of-bag-rmse
#       #rf$mse gives OOB mse for bagging 1:n trees, so last mse in this vector gives the OOB mse of the entire forest
#   
#       newdf <- data.frame(i,j,k,error)
#       metric.data <- rbind(metric.data, newdf)
#     }
#   }
#   print(i)
# }
# 
# colnames(metric.data) <- c("mtry","nodesize","resamplesize",
#                            "error")
# 
# write_csv(metric.data, "windrf.metric.data.csv")
```

We save this run as a csv so we don't have to rerun this lengthy optimization when we knit.

```{r}
metric.data1 <- read_csv("windrf.metric.data.csv")
metric.data1 <- metric.data1[2:391,] #remove 0,0,0,0 row
which.min.error <- which.min(metric.data1$error)
best.parameters <- metric.data1[which.min.error,]
print(paste("The most ideal hyperparameters are mtry = ", best.parameters$mtry, ", nodesize = ", best.parameters$nodesize, ", bootstrap resample size = ", best.parameters$resamplesize, ", with an error of", round(best.parameters$error)))
```

To optimize the random forest hyperparameters, all values of mtry were tested (1:13), where 13 is the number of possible input variables. For node size, only 1:10 were tested, and the 5 smallest error random forests had node sizes far below from the maximum tested value of 10. So, I decide not to test higher minimum node sizes. For the bootstrap resample size, I chose to test a range of values up to the maximum number of rows in the data set. I assumed a resample smaller than ~50% of my data set would not be effective. Ideally, I would test more granular values for this bootstrap resample size, but in the interest of time, I leave it at this. 

Now that we have our ideal mtry, node size, and resample size, all that's left is to choose ntree, or the number of trees. As noted earlier, more trees means less error, so let's see what a reasonable optimal ntree might be, considering run time. Let's compare the error from using ntree = 100 vs. ntree = 500.

```{r}
#try ntree = 100 (what was done above)
#we know error = 127, as was calculated above
system.time(rf100 <- randomForest(ActivePower ~ .,
                 data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 100))
```

Ntree = 100 took just over a minute to run, with an error of 127.

```{r}
#try ntree = 500
# rf500 <- randomForest(ActivePower ~ .,
#                  data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 500)
# 
# system.time(rf500 <- randomForest(ActivePower ~ .,
#                  data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 500))
# 
# tail(rf500$mse,1)
```

Running our random forest with 500 trees caused R to crash, so we are going to assume that 500 trees with our data set is too much for our computer to handle. Ideally we would have more than 100 trees in our forest, but we will have to stick with 100 trees to respect our computer's capabilities. 

The random forest model seems to predict more accurately, when compared to the kNN model. The kNN model doesn't seem super useful as using a k = 1 seems too simplistic. The random forest seems to be useful; at least it can tell us which variables were most important, which is interesting:

```{r}
#Our final tree
rf.final <- randomForest(ActivePower ~ .,
                 data = before.wind.rf, mtry = 9, nodesize = 1,sampsize = 10636, ntree = 100, importance = TRUE)
varImpPlot(rf.final)
```

It is fascinating that Reactive Power is the most important variable in our data set. Graphing Active Power vs. Reactive Power, this makes sense: 

```{r}
before.wind.rf%>%
  ggplot(mapping = aes(x = ReactivePower, y = ActivePower))+
  geom_point()
```

It is also fascinating that ambient temperature is a stronger predictor than wind speed, as one might guess wind speed to be more important. 

By looking at the small value of k in our kNN model, I would guess it would not transfer well to new data, as this model would have a high variance. For our random forest, that algorithm is known to have a relatively low variance. So, I would guess the random forest will transfer better to new data. To know for certain, we will have to test our two models on new data. 

____________________________________________________________________________________
##"After"

Let's see how our model performs on our "after" data. As a reminder, our "after" data is data read November 2019 - March 2020. 

First, we only select the uncorrelated variables from after.wind (the same columns we selected for before.wind):
```{r}
after.wind <- after.wind%>%
  dplyr::select(WindSpeed, WindDirection, AmbientTemperatue, BearingShaftTemperature, Blade1PitchAngle, GearboxBearingTemperature, GearboxOilTemperature, GeneratorRPM, GeneratorWinding1Temperature, HubTemperature, MainBoxTemperature, ReactivePower, datetime, tracking_id, ActivePower, TurbineStatus)
```

Now, let's predict active power in our after data with our kNN model:

```{r}
before.wind.kNN.no.power <- before.wind.kNN%>%
  dplyr::select(-ActivePower)
after.wind.kNN <- after.wind%>%
  dplyr::select(-datetime,-tracking_id,-TurbineStatus)
after.wind.kNN.no.power <- after.wind.kNN%>%
  dplyr::select(-ActivePower)
knn.model1 <- knn.reg(train = before.wind.kNN.no.power, 
                          test = after.wind.kNN.no.power, 
                          k = 1, y = before.wind.kNN$ActivePower)

after.wind.kNN <- after.wind.kNN%>%
      mutate(error = (after.wind.kNN$ActivePower - knn.model1$pred)^2)

mean.error.knn <- mean(after.wind.kNN$error)

mean.error.knn
```

Next, let's make active power predictions on our after data with our random forest:

```{r}
after.wind.rf <- after.wind%>%
  dplyr::select(-datetime,-tracking_id)
after.wind.rf.no.power <- after.wind.rf%>%
  dplyr::select(-ActivePower)
rf.preds <- predict(rf.final, after.wind.rf.no.power)
after.wind.rf <- after.wind.rf%>%
  mutate(preds = rf.preds)%>%
  mutate(error = (after.wind.rf$ActivePower - rf.preds)^2)

mean.error.rf <- mean(after.wind.rf$error)

mean.error.rf
  
```
Our kNN model had a mean squared error of around 2100 kW^2, which is higher than our previously estimated error of around 230 kW^2. The kNN model perfomed sqrt(9) = 3 times worse than what was estimated. This was expected though; as we noted previously, the low value of k used in our model, even though it was optimal, caused the model to have high variance. I was not expecting the kNN model to perform better on new data due to this feature. 

Our random forest had a mean squared error of ~14500 kW^2, which is way higher than the estimated error of 127 kW^2. The random forest performed sqrt(98) = ~10 times worse than expected. This is very surprising, as we thought the random forest would perform better than our kNN model, as it is more advanced and should have had a lower variance. I did not expect the random forest to behave that much worse than the estimated error from before. 

These results disagree with our estimations from before: the kNN model seems to perform better than the random forest.

What is going on with our random forest model? Is it just that we have strange outliers in our after.wind data? What values of active power did our models fail at predicting accurately?

```{r}
after.wind.rf%>%
  ggplot(mapping = aes(x = ActivePower, y = error))+
  geom_point()
```

It looks like the random forest performed progressively worse at predicting large active power values.

Out of curiousity, let's compare this to kNN:
```{r}
options(scipen = 999)
after.wind.kNN%>%
  ggplot(mapping = aes(x = ActivePower, y = error))+
  geom_point()
```

It looks kNN did a much better job at predicting larger values of active power, although it still predicted the lower values of active power more accurately, on average.

To explore why the models are performing so differently than expected on the after data, let's conduct Principle Components Analysis on both data sets. This might tell us if the underlying structure of the after data is drastically different from the before data.

```{r}
before.wind.vars <- before.wind%>%
  dplyr::select(WindSpeed,WindDirection,
                             AmbientTemperatue, BearingShaftTemperature, Blade1PitchAngle, GearboxBearingTemperature, GearboxOilTemperature,  
GeneratorRPM, GeneratorWinding1Temperature, HubTemperature, MainBoxTemperature, 
ReactivePower)

pca.before <- prcomp(before.wind.vars, scale = TRUE)
biplot(pca.before)

after.wind.vars <- after.wind%>%
  dplyr::select(WindSpeed,WindDirection,
                             AmbientTemperatue, BearingShaftTemperature, Blade1PitchAngle, GearboxBearingTemperature, GearboxOilTemperature,  
GeneratorRPM, GeneratorWinding1Temperature, HubTemperature, MainBoxTemperature, 
ReactivePower)

pca.after <- prcomp(after.wind.vars, scale = TRUE)
biplot(pca.after)
```

These biplots don't seem to look similar at all. It is hard to tell exactly which components are different for which variables from the biplot, given the number of variables.

Let's visualize the first principle components of each data set and see how they differ.

```{r}
pc1.values.before <- pca.before$rotation[,1]
pc1.values.after <- pca.after$rotation[,1]

pc1 <- data.frame(pc1.before = pc1.values.before,pc1.after = pc1.values.after)
pc1 <- pc1%>%
  mutate(variable = rownames(pc1))

pc1 <- pc1%>%
  gather(key = dataset, value = pc1.value, -variable)

pc1$dataset <- str_replace_all(pc1$dataset,"pc1.before","before")
pc1$dataset <- str_replace_all(pc1$dataset,"pc1.after","after")

pc1%>%
  ggplot(mapping = aes(x = variable, y = pc1.value, color = dataset))+
  geom_point()+
  theme(axis.text.x = element_text(angle = 45))

pc1%>%
  filter(dataset == "before")%>%
  arrange(-pc1.value)

pc1%>%
  filter(dataset == "after")%>%
  arrange(-pc1.value)
```

The first principle component values tell us there is around equal amounts of variance in the GearboxBearingTemperature and the GearboxOilTemperature in the before data, whereas in the after data it seems the GearboxBearingTemperature has a slighty larger amount of variance than GearboxOilTemperature. The GeneratorRPM has a higher variance in the after data than in the before data, given by its higher relative PC1 value. Reactive power is weighted relatively more heavily in the before data than in the after data. All of this is to say, the relative variances differ between the before and after data set, but the top 8 variables in both are weighted very similarly. This means these top 8 variables are consistently correlated across both data sets, which makes sense!

```{r}
plot(pca.before)
plot(pca.after)
```

It looks like the first principal component for both data sets has a variance of ~7. This means this first principal component explains only ~58% (= 7/12) of the variance in the data. Let's look at the second principle component to gain more insight. The first two components together should explain around 7 + 2.5 / 12 = ~80% of the variance in our data, which may prove useful.

```{r}
pc2.values.before <- pca.before$rotation[,2]
pc2.values.after <- pca.after$rotation[,2]

pc2 <- data.frame(pc2.before = pc2.values.before,pc2.after = pc2.values.after)

pc2 <- pc2%>%
  mutate(variable = rownames(pc2))

pc2 <- pc2%>%
  gather(key = dataset, value = pc2.value, -variable)

pc2$dataset <- str_replace_all(pc2$dataset,"pc2.before","before")
pc2$dataset <- str_replace_all(pc2$dataset,"pc2.after","after")

pc2%>%
  ggplot(mapping = aes(x = variable, y = pc2.value, color = dataset))+
  geom_point()+
  theme(axis.text.x = element_text(angle = 45))

pc2%>%
  filter(dataset == "before")%>%
  arrange(-pc2.value)

pc2%>%
  filter(dataset == "after")%>%
  arrange(pc2.value)
```

Looking at this second principle component, the MainBoxTemperature seems to have the highest amount of variance in the before data, while both MainBoxTemperature and AmbientTemperature are weighted around equally important for explaining most of the variance in the after data. 

Let's see what the distributions of the before and after data look like for some of the discussed variables, to see if we can notice any stark differences. 

```{r}
hist(before.wind$GearboxBearingTemperature)
hist(after.wind$GearboxBearingTemperature)
hist(before.wind$GearboxOilTemperature)
hist(after.wind$GearboxOilTemperature)

hist(before.wind$MainBoxTemperature)
hist(after.wind$MainBoxTemperature)
hist(before.wind$AmbientTemperatue)
hist(after.wind$AmbientTemperatue)

```

The distributions of the GearboxOilTemperature, MainBoxTemperature, and the AmbientTemperature all seem to be shifted to the left for the after data when compared to the before data. This makes sense as the before data is coming from seemingly warmer months (May 2019 through October 2019), whereas the after data is read during colder months (November 2019 - March 2020). Although we don't know the exact geographical location of the wind turbine, this tells us it is probably located in the northern hemisphere. 

Knowing that these temperature variables take on different distributions between our two data sets, it makes sense that the estimated error was lower than the actual error when tested on our after data, as the after data was simply taken in a different season. According to the EIA [1], in the United States at least, wind power generation tends to be the highest during the spring, lowest in the summer, and at a median value during the winter months. We can't necessarily apply these trends to this data set, as was mentioned before, we don't know exactly where the wind turbine is located. However, this does explain why the model did so poorly at predicting high active power generation values: perhaps the before data set contained a different amount of large active power generation values. 

```{r}
hist(before.wind$ActivePower)
hist(after.wind$ActivePower)
```

This is confirmed: the before data set and active data set's power generation distributions seem to be very different. This makes sense, given the change in seasons! 

Finally, let's make a new model that combines both the before and after data. 

Before we do so, we want to identify and get rid of outliers in the after data set using hierarchical clustering. We already got rid of outliers in the before data.

```{r}
hc2 <- after.wind%>%
  dplyr::select(-c(tracking_id, datetime, TurbineStatus, ActivePower))%>%
  scale()%>%
  dist()%>%
  hclust()

after.wind.clusters <- after.wind%>%
  mutate(cluster = cutree(hc2, k = 5)) #this k is arbitrary

hist(after.wind.clusters$cluster)
```

It looks like clusters 4 and 5 are standing out from the rest of the after data. What is going on here?

```{r}
after.wind.clusters%>%
  group_by(cluster)%>%
  summarize_all(mean)

#How many points are in each cluster?
after.wind.clusters%>%
  filter(cluster == 1)%>%
  nrow()

after.wind.clusters%>%
  filter(cluster == 2)%>%
  nrow()

after.wind.clusters%>%
  filter(cluster == 3)%>%
  nrow()

after.wind.clusters%>%
  filter(cluster == 4)%>%
  nrow()

after.wind.clusters%>%
  filter(cluster == 5)%>%
  nrow()
```

It looks like clusters 4 and 5 have the lowest Active Power generation. It also looks like the average month for clusters 4 and 5 was February and December respectively, with the other three clusters being mostly in January. I'm not seeing any average values in these two clusters (clusters 4 and 5) that are absurdly different from the others, so I'm still going to use all the clusters to build my model. 

For our final model, let's use the kNN algorithm. Although this model may have a higher variance than a random forest, it predicted with a smaller amount of error compared to the random forest via both cross-validation and testing on the after data. Also, our only categorical variable, TurbineStatus, did not seem crucial to our model (at least for our rf.final, looking at the variable importance plot), so we don't believe leaving it out will make that much of a difference.

```{r}
after.wind.kNN <- after.wind.kNN%>%
  dplyr::select(-error) #remove error col so we can combine data sets
all.wind.kNN <- rbind(before.wind.kNN, after.wind.kNN)
```

We will optimize our choice of k using k-fold cross-validation, with 5 folds, as before. Our error metric is the same as before.

```{r}
folds <- 5
fold.error.vector <- NULL
error.vector <- NULL

wind.data.groups <- split(all.wind.kNN, sample(1:folds, nrow(all.wind.kNN), replace=T))

for (k in 1:100){
  for (fold in 1:folds){
    myfold <- data.frame(wind.data.groups[fold])
    colnames(myfold) <- colnames(all.wind.kNN)
    test.data <- myfold%>%
      subset(select = -ActivePower)
    training.data <- all.wind.kNN[-as.numeric(rownames(test.data)),]
    training.data.no.power <- training.data %>% 
      subset(select = -ActivePower)

    model1 <- knn.reg(train = training.data.no.power, 
                          test = test.data, 
                          k = k, y = training.data$ActivePower)
    test.data <- test.data%>%
      mutate(error = (myfold$ActivePower - model1$pred)^2)
    fold.error.vector[fold] <- mean(test.data$error) #stores average error value for a given fold
  }
  error.vector[k] <- mean(fold.error.vector) #stores average of average error value of all folds for a given k
}
```

```{r}
pred.data <- data.frame(error.vector, 
                        k = 1:100)
pred.data %>%
  ggplot(aes(x = k, 
             y = error.vector)) +
  geom_point() +
  ggtitle("Error vs. K using kNN") +
  xlab("K Value") +
  ylab("Error") +
  theme(plot.title = element_text(hjust = 0.5))

k.best <- which.min(error.vector)
error.best <- pred.data$error.vector[k.best]
print(paste("The best k value to use is k =",as.character(k.best),"with a minimal mean error of",as.character(round(error.best))))
```

My final model uses the given kNN algorithm, with k = 1, as shown above. Perhaps not too surprising, this final model has the same optimal k value as was found when we only trained on the before data. Although this model will have a high variance due to the low value of k, I believe it will perform well on new data, based on what I have seen so far. With only looking at half of the year's wind turbine data, the model only had 3 more times the amount of error when predicting the after data's active power variable. But, now that the kNN algorithm has seen data coming from all seasons, I would bet it will most likely have at maximum a mean squared error of around 2100 kW^2 for new data (given the new data is not riddled with outliers). This is a mean error of only sqrt(2100) = 45kW.

```{r}
mean.power <- mean(all.wind.kNN$ActivePower)
45/mean.power
```
This estimated mean error is very good considering the average active power is around 607 kW - the error is only around 7% of the average active power!

But, this error estimation is, of course, just an estimation. We will have to test our model on additional new data to find out the true error.

Further research questions we could ask include: 
-How would our final model perform predicting active power for an arbitrary wind turbine, of any height / size? 
-How would our final model perform for the same turbine located in a different part of the world? 
-Is the model that we built useful for real applications? 
-Who might this model be relevant to? Utility companies? Wind turbine owners? Economists?
-Which variables in the data set will be known in advance, and which ones are only retroactively knowable? For example, I would suppose the Reactive Power variable would not be known until after the active power was already generated, in which case it could not be used to predict future power output.

[1] Source: https://www.eia.gov/todayinenergy/detail.php?id=20112 
